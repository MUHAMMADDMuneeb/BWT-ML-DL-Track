{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture: Hybrid U-Net for Microaneurysm Detection\n",
    "Architecture Overview: The Hybrid U-Net architecture was developed to enhance the traditional U-Net model by integrating additional components that improve feature learning and segmentation accuracy, especially in challenging tasks like microaneurysm detection in retinal fundus images. This hybrid model leverages the strengths of U-Net while incorporating elements such as attention mechanisms and residual connections to capture more complex patterns and refine segmentation results.\n",
    "\n",
    "Flow of Data within the Model:\n",
    "The Hybrid U-Net architecture maintains the core structure of U-Net with a contracting path (encoder) and an expanding path (decoder), while introducing modifications that enhance feature extraction and spatial information retention.\n",
    "\n",
    "Contracting Path (Encoder):\n",
    "The encoder captures the contextual information from the input image through a series of convolutional layers, similar to the traditional U-Net. However, the Hybrid U-Net incorporates residual blocks instead of simple convolutional layers. Each residual block consists of two 3x3 convolutional layers followed by ReLU activations, with a skip connection that adds the input of the block to its output. This structure helps in mitigating the vanishing gradient problem and allows for deeper networks.\n",
    "\n",
    "Each convolution operation in the encoder continues to double the number of feature channels, capturing increasingly abstract and high-level features as the spatial dimensions decrease.\n",
    "\n",
    "Bottleneck:\n",
    "At the bottleneck, the model processes the image through two 3x3 convolutions with ReLU activations. The bottleneck also integrates an attention mechanism, which focuses on the most relevant features for segmentation by recalibrating the feature maps based on their importance. This attention mechanism is critical in highlighting subtle features like microaneurysms.\n",
    "\n",
    "Expanding Path (Decoder):\n",
    "The decoder in the Hybrid U-Net aims to restore the spatial resolution of the feature maps while ensuring that the high-level features captured by the encoder are effectively utilized. It employs transposed convolutions for upsampling, followed by the integration of feature maps from the encoder through skip connections.\n",
    "\n",
    "To further enhance the model's performance, the Hybrid U-Net uses attention gates at each skip connection. These gates selectively filter the encoder’s feature maps, allowing the decoder to focus on the most relevant information for segmentation.\n",
    "\n",
    "Output Layer:\n",
    "The final layer remains a 1x1 convolution, reducing the number of output channels to match the number of classes (one for microaneurysms). A sigmoid activation function is applied to generate a pixel-wise binary classification map, identifying the presence of microaneurysms.\n",
    "\n",
    "Choice of Optimizers and Metrics:\n",
    "Optimizer:\n",
    "The Adam optimizer was retained for the Hybrid U-Net due to its effectiveness in deep learning tasks. Adam's adaptive learning rate capabilities ensure faster convergence, which is crucial when training a more complex hybrid model. The optimizer’s ability to handle varying gradient scales is especially beneficial in the context of the Hybrid U-Net, where multiple architectural components interact.\n",
    "\n",
    "Loss Function:\n",
    "Binary Cross-Entropy (BCE): The model continues to use Binary Cross-Entropy as the loss function. BCE is ideal for this binary classification task, as it quantifies the dissimilarity between the predicted microaneurysm map and the ground truth, guiding the model towards accurate segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "dataset_path = \"F:/Fyp/Preprocessing/1_Microaneurysms/Processed\"\n",
    "image_dir = os.path.join(dataset_path, \"Images\")\n",
    "mask_dir = os.path.join(dataset_path, \"Mask\")\n",
    "\n",
    "# Image dimensions\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "IMG_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_and_masks(image_dir, mask_dir):\n",
    "    images = sorted(glob.glob(os.path.join(image_dir, \"*\")))\n",
    "    masks = sorted(glob.glob(os.path.join(mask_dir, \"*\")))\n",
    "\n",
    "    images = [cv2.imread(img) for img in images if os.path.isfile(img)]\n",
    "    masks = [cv2.imread(mask, cv2.IMREAD_GRAYSCALE) for mask in masks if os.path.isfile(mask)]\n",
    "\n",
    "    # Resize images and masks to the desired size\n",
    "    images = [cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT)) for img in images]\n",
    "    masks = [cv2.resize(mask, (IMG_WIDTH, IMG_HEIGHT)) for mask in masks]\n",
    "\n",
    "    images = np.array(images)\n",
    "    masks = np.expand_dims(np.array(masks), axis=-1)  # Add channel dimension\n",
    "\n",
    "    # Normalize images and masks\n",
    "    images = images / 255.0\n",
    "    masks = masks / 255.0\n",
    "\n",
    "    return images, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Dense, Add\n",
    "\n",
    "def transformer_block(x, num_heads, key_dim, ff_dim, rate=0.1):\n",
    "    # Multi-Head Self Attention\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)\n",
    "    attn_output = Add()([x, attn_output])  # Skip connection\n",
    "    attn_output = LayerNormalization(epsilon=1e-6)(attn_output)\n",
    "\n",
    "    # Feed Forward Network\n",
    "    ffn_output = Dense(ff_dim, activation='relu')(attn_output)\n",
    "    ffn_output = Dense(x.shape[-1])(ffn_output)\n",
    "    ffn_output = Add()([attn_output, ffn_output])  # Skip connection\n",
    "    ffn_output = LayerNormalization(epsilon=1e-6)(ffn_output)\n",
    "    \n",
    "    return ffn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_unet_model(input_size=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)):\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
    "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "    # Bottleneck layer\n",
    "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
    "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    # Integrate Transformer Block\n",
    "    c5_transformed = transformer_block(c5, num_heads=8, key_dim=64, ff_dim=1024)\n",
    "\n",
    "    u6 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5_transformed)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
    "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
    "\n",
    "    u7 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
    "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
    "\n",
    "    u8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
    "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
    "\n",
    "    u9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
    "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "images, masks = load_images_and_masks(image_dir, mask_dir)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(images, masks, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "data_gen_args = dict(rotation_range=15,\n",
    "                     width_shift_range=0.1,\n",
    "                     height_shift_range=0.1,\n",
    "                     shear_range=0.1,\n",
    "                     zoom_range=0.1,\n",
    "                     horizontal_flip=True,\n",
    "                     fill_mode='nearest')\n",
    "\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "# Create generators\n",
    "train_image_generator = image_datagen.flow(train_images, batch_size=16, seed=42)\n",
    "train_mask_generator = mask_datagen.flow(train_masks, batch_size=16, seed=42)\n",
    "\n",
    "# Create the combined dataset from the generators\n",
    "def generator_to_dataset(image_gen, mask_gen):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: zip(image_gen, mask_gen),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(16, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(16, IMG_HEIGHT, IMG_WIDTH, 1), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "train_dataset = generator_to_dataset(train_image_generator, train_mask_generator)\n",
    "\n",
    "# Apply prefetching to the dataset\n",
    "train_dataset = train_dataset.repeat().prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5481s\u001b[0m 274s/step - accuracy: 0.8171 - loss: 0.2579 - val_accuracy: 0.9892 - val_loss: 0.0152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 37s/step - accuracy: 0.9902 - loss: 0.0131\n",
      "Validation Accuracy: 98.92%\n"
     ]
    }
   ],
   "source": [
    "# Create the hybrid U-Net model\n",
    "hybrid_model = hybrid_unet_model()\n",
    "\n",
    "# Train the model\n",
    "history = hybrid_model.fit(train_dataset,\n",
    "                    steps_per_epoch=len(train_images) // 16,\n",
    "                    validation_data=(val_images, val_masks),\n",
    "                    epochs=1)\n",
    "\n",
    "# Save the trained model\n",
    "hybrid_model.save('hybrid_unet_model.h5')\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "loss, accuracy = hybrid_model.evaluate(val_images, val_masks)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
