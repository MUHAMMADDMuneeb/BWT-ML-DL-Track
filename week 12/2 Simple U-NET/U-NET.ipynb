{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture: U-Net for Microaneurysm Detection<br>\n",
    "Architecture Overview: The U-Net architecture was chosen for this project due to its proven effectiveness in medical image segmentation tasks, particularly in scenarios where precise localization of features like microaneurysms is crucial. U-Net is a fully convolutional network (FCN) that excels in handling small datasets and provides high accuracy in segmenting complex structures within images.<br>\n",
    "\n",
    "#### Flow of Data within the Model:\n",
    " The U-Net architecture consists of two main parts: the contracting path (encoder) and the expanding path (decoder).\n",
    "<br>\n",
    "\n",
    "#### Contracting Path (Encoder):<br>\n",
    "\n",
    "The encoder is responsible for capturing the context of the input image. It follows a typical convolutional neural network (CNN) structure, comprising repeated application of two 3x3 convolutional layers followed by a ReLU activation and a 2x2 max-pooling operation for downsampling.<br>\n",
    "Each convolution operation doubles the number of feature channels, allowing the network to capture increasingly abstract and high-level features as the spatial dimensions of the image reduce.<br>\n",
    "#### Bottleneck:\n",
    "<br>\n",
    "At the bottleneck of the network, the image is represented by a highly abstract feature map with a reduced spatial dimension but a rich feature set. This stage consists of two 3x3 convolutions with ReLU activations, followed by another 2x2 max-pooling operation<br> \n",
    "\n",
    "#### Expanding Path (Decoder):\n",
    "\n",
    "The decoder is designed to recover the spatial resolution of the image, effectively upsampling the encoded features back to the original input size. It uses transposed convolutions (also known as deconvolutions) to upsample the feature maps.<br>\n",
    "Each upsampling step is followed by a concatenation with the corresponding feature map from the encoder (skip connections). These skip connections allow the network to retain high-resolution information that might otherwise be lost during downsampling.<br>\n",
    "#### Output Layer:<br>\n",
    "\n",
    "The final layer is a 1x1 convolution that reduces the number of output channels to the desired number of classes (in this case, one for microaneurysms), followed by a sigmoid activation function to produce a pixel-wise binary classification map.\n",
    "Choice of Optimizers and Metrics:\n",
    "<br>\n",
    "\n",
    "#### Optimizer:\n",
    "<br>\n",
    "The Adam optimizer was chosen for this model due to its adaptive learning rate capabilities, which help in faster convergence and robustness to varying gradient scales. Adam combines the advantages of two other popular optimizers—AdaGrad and RMSProp—making it particularly effective for training deep networks like U-Net.\n",
    "<br>\n",
    "\n",
    "#### Loss Function:\n",
    "<br>\n",
    "Binary Cross-Entropy (BCE): Given the binary nature of the task (microaneurysm present or not), Binary Cross-Entropy is used as the loss function. BCE is well-suited for binary classification tasks, providing a measure of dissimilarity between the predicted and true binary labels.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3180s\u001b[0m 157s/step - accuracy: 0.9634 - loss: 0.8835 - val_accuracy: 0.9892 - val_loss: 0.0281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 38s/step - accuracy: 0.9902 - loss: 0.0250\n",
      "Validation Accuracy: 98.92%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "\n",
    "# Paths\n",
    "dataset_path = \"F:/Fyp/Preprocessing/1_Microaneurysms/Processed\"\n",
    "image_dir = os.path.join(dataset_path, \"Images\")\n",
    "mask_dir = os.path.join(dataset_path, \"Mask\")\n",
    "\n",
    "# Image dimensions\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "# Load images and masks\n",
    "def load_images_and_masks(image_dir, mask_dir):\n",
    "    images = sorted(glob.glob(os.path.join(image_dir, \"*\")))\n",
    "    masks = sorted(glob.glob(os.path.join(mask_dir, \"*\")))\n",
    "\n",
    "    images = [cv2.imread(img) for img in images if os.path.isfile(img)]\n",
    "    masks = [cv2.imread(mask, cv2.IMREAD_GRAYSCALE) for mask in masks if os.path.isfile(mask)]\n",
    "\n",
    "    # Resize images and masks to the desired size\n",
    "    images = [cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT)) for img in images]\n",
    "    masks = [cv2.resize(mask, (IMG_WIDTH, IMG_HEIGHT)) for mask in masks]\n",
    "\n",
    "    images = np.array(images)\n",
    "    masks = np.expand_dims(np.array(masks), axis=-1)  # Add channel dimension\n",
    "\n",
    "    # Normalize images and masks\n",
    "    images = images / 255.0\n",
    "    masks = masks / 255.0\n",
    "\n",
    "    return images, masks\n",
    "\n",
    "# Define U-Net model\n",
    "def unet_model(input_size=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)):\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
    "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
    "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    u6 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
    "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
    "\n",
    "    u7 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
    "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
    "\n",
    "    u8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
    "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
    "\n",
    "    u9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
    "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Load the data\n",
    "images, masks = load_images_and_masks(image_dir, mask_dir)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data augmentation\n",
    "data_gen_args = dict(rotation_range=15,\n",
    "                     width_shift_range=0.1,\n",
    "                     height_shift_range=0.1,\n",
    "                     shear_range=0.1,\n",
    "                     zoom_range=0.1,\n",
    "                     horizontal_flip=True,\n",
    "                     fill_mode='nearest')\n",
    "\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "# Create generators\n",
    "train_image_generator = image_datagen.flow(train_images, batch_size=16, seed=42)\n",
    "train_mask_generator = mask_datagen.flow(train_masks, batch_size=16, seed=42)\n",
    "\n",
    "# Create the combined dataset from the generators\n",
    "def generator_to_dataset(image_gen, mask_gen):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: zip(image_gen, mask_gen),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(16, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(16, IMG_HEIGHT, IMG_WIDTH, 1), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "train_dataset = generator_to_dataset(train_image_generator, train_mask_generator)\n",
    "\n",
    "# Apply prefetching to the dataset\n",
    "train_dataset = train_dataset.repeat().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Create the U-Net model\n",
    "model = unet_model()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset,\n",
    "                    steps_per_epoch=len(train_images) // 16,\n",
    "                    validation_data=(val_images, val_masks),\n",
    "                    epochs=1)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('unet_model.h5')\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "loss, accuracy = model.evaluate(val_images, val_masks)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
